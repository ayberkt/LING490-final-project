\documentclass{article}
\usepackage{natbib}
\usepackage{todonotes}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{mathrsfs}

\newcommand{\hmmURL}{https://hackage.haskell.org/package/hmm-0.2.1.1/docs/Data-HMM.html}

\title{\bf An HMM-based part-of-speech tagger for Turkish}
\author{Ayberk Tosun\\\texttt{tosun2@illinois.edu}}
\date{}

\begin{document}
%include agda.fmt
\maketitle

\section{Motivation}
\label{sec:motivation}

The main motivation for this project comes from the lack of a POS tagger for
Turkish exclusively implemented in the programming language Haskell. Moreover, I
believe that implementing a POS tagger from scratch is beneficial in terms of
gain experience with relevant statistical models through this process.

Due to time and resource constraints, it is not plausible to build a
state-of-the-art POS tagger for Turkish. Nevertheless, I expect my POS tagger to
work with decent accuracy that is adequate to be a starting point for further
development. Moreover, \citet{Korkut2015} began the development of a Turkish NLP
library named ``Guguk'', implemented in the programming language Haskell. I
intend to merge this project to Guguk, hence contributing towards the existence
of a Haskell Turkish NLP library; I intend to continue development of this
project as a part of Guguk.

Before I proceed, let me give a few remarks on notational conventions I follow
throughout this text: I use \texttt{HMM} (with the fixed-width font) to the
datatype I'm using from the \texttt{Data.HMM} package and HMM (with regular
font) to refer to the concept.

\section{Related Work}
\label{sec:related_work}

% Briefly survey the most salient prior work that relates to the problem you wish
% to solve. This section should cite relevant sources. This section should be at
% least one to two paragraphs in length.
\cite{dincer2008suffix} built a POS tagger for Turkish using HMMs on suffixes.
This proves an effective approach seeing that (1) Turkish morphology is
exclusively based on suffixes, in contrast to to prefixes (2) morphology is
central to the Turkish language so it is likely that any given word is formed
through the agglutination of at least severals morphemes. The tagger described
by \citet{dincer2008suffix} has ``\%90.2 accuracy in the best case''.

The widely cited \citet{oflazer1994tagging} outlines the implementation of a
tool for both morpheme glossing and POS tagging, that has ``98-99 \%'' accuracy.
This tool is based on a morphological specification of Turkish and makes use of
a morphological parser called ``PC-Kimmo'' \citep{antworth1991pc}. I should
stress that \citet{oflazer1994tagging} does POS tagging as a by-product of its
capability of morphological parsing.

In general, most of the literature relating to the computational processing of
Turkish pertain primarily to morphology. Compared to the complexity of Turkish
morphology, its syntax seems to be a less interesting problem.

\section{Proposed Work}

I will implement a part-of-speech tagger for the Turkish language. As mentioned
in section~\ref{sec:related_work}, I will mostly focus on parts-of-speech of
words\footnote{I base my definition of a ``word'' on the orthographic space in
  most of the cases. There are certain exceptions to this, such as duplication
  which I denote with the POS tag ``Dup''} and will not be accounting for
morphology, unlike most of the related work I've mentioned. There are definite
problems with this approach; fixing these would go beyond the scope of this
paper therefore I defer the integration of morphology for further development.

For programming the POS tagger I will be using Haskell, a statically-typed and
purely functional programming language, along with the
\href{\hmmURL}{\texttt{Data.HMM}} package. The data, that is probably the most
important component of this project, comes from the METU-Sabanc{\i} treebank
built by \citet{oflazer2003building}. I am planning to strip the POS tag for
each word from the treebank and use those to train the HMM

\section{Results}

The resulting program I have implemented is based on bigram hidden markov model.
To summarize it, I am making use of three probabilities: (1) $P(t_i | t_{i-1})$
the probability of being at a state (i.e., a tag) given the previous state. This
is computed\footnote{Although I use the word ``compute'', everything is already
  pre-computed when a call to this function is made and the referred computation
  is merely a lookup. This is discussed in \ref{par:tag}} by the function
\texttt{transFn}. (2) $P(w_i | t_i)$ the probability of seeing a word with a
given tag, computed by the function \texttt{outFn}. Finally, the probability of
a given state being the start state of a sentence, computed by the function
\texttt{outProbs}.

\subsection{A Brief Overview of the Program}

The project consists of two executables: (1) \texttt{create-model} that reads
the data from the corpus and parses it into a form from which we can easily
construct an \texttt{HMM}. Unfortunately, it is highly impractical---if not
impossible---to directly serialize the \texttt{HMM} datatype since it has three
fields containing functions \footnote{See
  http://stackoverflow.com/questions/17785916/can-haskell-functions
  -be-serialized for further information.}. We consider this a fault in the
design of the \texttt{Data.HMM} package and urge further development; this has
almost no effect on performance, but it obfuscates the code.

Since processing the corpus is a resource-demanding operation,
\texttt{create-model} is intended be run only once after which \texttt{run-tag}
can be used to make use of the serialized components of the HMM.

I should note that it is not possible to give a complete account of the
program's logic in this chapter although we explain the most salient components.
The source code is in the \texttt{ayberkt-final_project} repository and it is
well-commented. Should the reader have any concerns regarding implemenetation
they could find it convenient to consult the source code.

\subsubsection{\texttt{create-model}}

The executable \texttt{create-model} is implemented in two files:
\texttt{Parse.hs} and \texttt{Main.hs}. \texttt{Parse.hs} handles the parsing of
the treebank.

The function \texttt{freqMap} found in \texttt{create/Main.hs} takes as input a
list of orderable elements (denoted with \texttt{Ord a}) and creates a
\texttt{Map} containing, as value, the number of ocurrence of each element.

\begin{lstlisting}[
  language=Haskell,
  mathescape,
  columns=fullflexible,
  basicstyle=\small\ttfamily
]
freqMap :: Ord a $\Rightarrow$ [a] $\rightarrow$ M.Map a Int
freqMap elems = populate empty elems
  where populate :: Ord a Map a Int $\rightarrow$ [a] $\rightarrow$ M.Map a Int
         populate m [] = m
         populate m (x:xs) = let freq = (M.findWithDefault 0 x m) :: Int
                              in populate (M.insert x (freq + 1) m) xs
\end{lstlisting}

This function is called several times with lists of values we want to
\emph{count} (such as the list of tag bigrams, or the list of all tagged words)
et cetera. Pre-computing the counts is almost mandatory since frequencies (i.e.,
counts) will need to be accessed many times by the viterbi algorithm when
finding the most probable path through the model. Once the necessary maps are
generated, they are written to files in the directory \texttt{model}.

\subsubsection{\texttt{tag}}
\label{par:tag}

The executable \texttt{run-tag} is where the actual tagging happens. The
frequencies we have pre-computed in \texttt{create-model} are passed in to the
probability function which we give below.
\begin{lstlisting}[
  language=Haskell,
  mathescape,
  columns=fullflexible,
  basicstyle=\small\ttfamily
]
probability :: (Ord a, Ord b) $\Rightarrow$ (a, b) $\rightarrow$ Map a Int $\rightarrow$ Map (a, b) ℤ $\rightarrow \mathscr{L}$
probability (x, y) c$_1$ c$_2$ = if xCount == 0 || yCount == 0
                                   then logFloat $\epsilon$
                                   else (logFloat yCount) / (logFloat xCount)
  where yCount = fromIntegral (findWithDefault 0 (x, y) c₂) ∷ Double
        xCount = fromIntegral (findWithDefault 0 x c₁) ∷ Double
\end{lstlisting}
This is a straightforward implementation of probability relying on a given count
map. One thing to note is that it returns $\epsilon$ probability (i.e., the
smallest value a \texttt{LogFloat}\footnote{A float in the log domain.} can have) if a given element is not found.

\texttt{probability} when combined with \texttt{freqMap} forms an efficient
method for implementing dependent probability.

When we are creating the functions required by \texttt{HMM} we are storing the
map returned by a call to \texttt{freqMap} in the lexical closure of
\texttt{probability} by making use Haskell's currying so that we can eloquently
define the two crucial functions as follows:
\begin{lstlisting}[
  language=Haskell,
  mathescape,
  columns=fullflexible,
  basicstyle=\small\ttfamily
]
    transFn s$_1$ s$_2$ = probability (s$_1$, s$_2$) tagFreqs tagBigramFreqs
    outFn s e = probability (e, s) wordFreqs taggedWordFreqs
\end{lstlisting}
The probability functions used to create \texttt{transFn} and \texttt{outFn}
hold the count maps \texttt{tagBigramFreqs} and \texttt{taggedWordFreqs} in
their lexical closure, respectively.

Finally we declare a new \texttt{HMM} as follows:
\newpage
\begin{lstlisting}[
  language=Haskell,
  mathescape,
  columns=fullflexible,
  basicstyle=\small\ttfamily
]
      newHMM = HMM { states      = possibleTags ∷ [POS]
                     , events      = ws ∷ [String]
                     , initProbs   = initProbFn
                     , transMatrix = transFn
                     , outMatrix   = outFn
                     }
\end{lstlisting}

This is a simple record\footnote{A record is a datatype declaration in Haskell
  similar to a struct in C.} containing the probability functions we
defined.\footnote{For some reason, the author of the \texttt{Data.HMM} library
  named the fields (i.e., attribute), that are of function types, using names
  that end with \texttt{-Matrix} although they are \emph{not} matrices. The
  reader should not be confused by this.}



\subsection{Performance}
Parsing the entire corpus and creating a model out of it takes $10.5$ seconds on
average.\footnote{On my i5 MacBook} Tagging a sentence grows with $O(n)$
complexity where $n$ is the number of words in a given sentence. Assuming a
sentence has 5 words on average, this operation takes around $~0.3$ seconds.
\footnote{https://hackage.haskell.org/package/array-0.5.1.0/docs/Data-Array.html}

\section{Conclusion}

We have shown that it is possible---though not ideal---to build a POS tagger for
Turkish---a canonical agglutinative language---that is based entirely on a
bigram HMM. In this section I analyze the accuracy of the tagger and discuss
simple improvements I can make, as well as its fundamental limitations fixing
which would require a significant rewrite of the tagger. Let us first look at
the accuracy of this model and propose possible fixes that do not fundamentally
change the structure of the model.

\subsection{Accuracy}
\label{subsec:accuracy}

We have tested the POS tagger with 6075 words from a small portion of the corpus
that I reserved for testing (i.e., did not use for tranining the HMM). The POS
tagger tagged 4514 of these words correctly (74.3 \%). Here is a brief list of
known problems:

\begin{description}

  \item[Duplications:] The Turkish language contains many duplications.
  Syntactically speaking, a duplication behaves like a word itself. In the
  treebank, duplications are denoted with two halves joint the with an
  underscore. Since there is not an easy way to detect whether two sequential
  words are reduplications or not, we are giving duplications in an incorrect
  format to the HMM.  We predict that (based on informal tests with the model)
  that correction of this problem would significantly increase the accuracy.

  \item[Abundance of independent clauses:] It is common in the Turkish language
  to have multiple independent clauses in a given sentence. It is possible that
  this decreases the accuracy of the HMM: instead of a tag $t$ being followed by
  a tag $t'$, we get a lot of $t, <clause>, t'$ which obscures the word order we
  are interested in. Since we may always assign the \textsc{Punc} tag to a comma
  with full certainty, we may simply ignore commas as far as probability is
  concerned and tag them with \textsc{Punc}.

\end{description}

\subsection{Limitations}
There exists a fundamental limitation to the described tagger: Turkish
morphology is essential to the overall completeness of Turkish.

In English we entertain the notion of \emph{syntactic correctness}, which is
more or less a formalism of the correct ordering of words. A speaker of English
that has synactic competence in the language \citep{bachman1990} may know that a
sentence is well-formed, relying on his knowledge of syntactic rules. However,
for the \textit{words}, knowledge of morphological rules is not adequate to
conclude that a word is made up of correctly ordered morphemes; they have to
rely on their lexical knowledge to verify that the morphological form they
derived/inflected is a valid one. Consider the word ``satisfiableness'' which is
not a valid word according to several dictionaries, although being derived with
valid suffixes. The correct form of this concept is ``satisfiability''.

In Turkish, the concept of word validness is a lot like syntactic correctness;
the speaker may rely their knowledge of morphological rules to \emph{generate}
new words. In fact Turkish has \textbf{recursive} morphology \citep{kabak2001}
so a word can be inflected/derived in into infinitely many different forms. This
is why computational processing of Turkish can never fully escape its
morphology.

\subsection{Further research}

A salient possibility for further research is the incorporation of word endings
into the model using regular expressions. It is trivial to express Turkish
suffixes, that change their form as not to upset the major vowel harmony in a
given word. We may say with probably more than $90\%$ probability that a given
word has a certain POS looking whether its ending satisfies regular expressions.

\bibliographystyle{apalike}
\bibliography{your_bibliography}

\end{document}