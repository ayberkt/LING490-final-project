\documentclass{article}
\usepackage{natbib}
\usepackage{todonotes}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{mathrsfs}

\newcommand{\hmmURL}{https://hackage.haskell.org/package/hmm-0.2.1.1/docs/Data-HMM.html}

\title{\bf An HMM-based part-of-speech tagger for Turkish}
\author{Ayberk Tosun\\\texttt{tosun2@illinois.edu}}
\date{}

\begin{document}
%include agda.fmt
\maketitle

\section{Motivation}
\label{sec:motivation}
My main motivation for undertaking this project is to understand the
principles underlying POS-tagging. Due to time and resource constraints, it is
not possible for me to build a state-of-the-art POS-tagger for
Turkish. Nevertheless, I expect my POS-tagger
to work with a decent accuracy that is adequate to be a starting point for
further development. Moreover, \citet{Korkut2015} began the development of a
Turkish NLP library named ``Guguk'', implemented in the programming language Haskell. I intend
to merge this project to Guguk, hence contributing towards
the existence of a Turkish NLP-library in Haskell.

Before proceeding, a few remarks on notational conventions: I use
\texttt{HMM} (with the fixed-width font) to the datatype I'm using from the
\texttt{Data.HMM} package and HMM (with regular font) to refer to the concept.

\section{Related Work}
\label{sec:related_work}

% Briefly survey the most salient prior work that relates to the problem you wish
% to solve. This section should cite relevant sources. This section should be at
% least one to two paragraphs in length.
\cite{dincer2008suffix} built a POS tagger for Turkish using HMMs on suffixes.
This proves an effective approach seeing that (1) Turkish morphology is
exclusively based on suffixes (not prefixes) (2) morphology is central to the
Turkish language so it is likely that any given word is formed through the
agglutination of at least one morpheme. The tagger described has ``\%90.2
accuracy in the best case''.

The widely cited \citet{oflazer1994tagging} outlines the implementation of a
tool for both morpheme glossing and POS tagging, that has ``98-99 \%'' accuracy.
This tool is based on a morphological specification of Turkish and makes use of
a morphological parser called ``PC-Kimmo'' \citep{antworth1991pc}. I should
stress that \citet{oflazer1994tagging} does POS tagging as a by-product of its
capability of morphological parsing.

In general, most of the literature relating to the computational processing of
Turkish pertain primarily to morphology. Compared to the complexity of Turkish
morphology, its syntax seems to be a less interesting problem.

\section{Proposed Work}

I will implement a part-of-speech tagger for the Turkish language. As mentioned
in section~\ref{sec:related_work}, I will mostly focus on parts-of-speech of
words\footnote{I base my definition of a ``word'' on the orthographic space in
  most of the cases. There are certain exceptions to this, such as duplication
  which I denote with the POS tag ``Dup''} and will not be accounting for
morphology, unlike most of the related work I've mentioned. There are definite
problems with this approach. Fixing these would go beyond the scope of this
paper therefore I defer the integration of morphology for further development.

For programming the POS tagger I will be using Haskell, a statically-typed and
purely functional programming language, along with the
\href{\hmmURL}{\texttt{Data.HMM}} package. The data, that is probably the most
important component of this project, comes from the METU-Sabanc{\i} treebank
built by \citet{oflazer2003building}. I am planning to strip the POS tag for
each word from the treebank and use those to train the HMM

\section{Results}
The POS tagger I've built as proposed can tag words with a
reasonable accuracy which we talk about in \ref{subsec:accuracy}.

\subsection{A Brief Overview of the Program}
The project consists of two executables: (1) \texttt{create-model} that reads
the data from the corpus and parses it into a form from which we can easily
construct an \texttt{HMM}. Unfortunately, it is highly impractical---if not
impossible---to directly serialize \texttt{HMM} since it has three fields
containing functions
\footnote{http://stackoverflow.com/questions/17785916/can-haskell-functions
  -be-serialized for further information.}. We consider this a fault in the
design of the \texttt{Data.HMM} package and urge further development. Although
this has almost no effect on performance, it obfuscates the code.

Since processing the corpus is a resource-demanding operation,
\texttt{create-model} is intended be run only once after which \texttt{run-tag}
can be used to make use of that data.

\paragraph{\texttt{create-model}}
The executable \texttt{create-model} is implemented in two files:
\texttt{Parse.hs} and \texttt{Main.hs}. \texttt{Parse.hs} handles the parsing of
the corpus.

The function \texttt{freqMap} takes as input a list of orderable elements
(denoted with \texttt{Ord a}) and creates a \texttt{Map} containing, as value,
the frequency of each element.

\begin{lstlisting}[
  language=Haskell,
  mathescape,
  columns=fullflexible,
  basicstyle=\small\ttfamily
]
freqMap :: Ord a $\Rightarrow$ [a] $\rightarrow$ M.Map a Int
freqMap unigrams = populate M.empty unigrams
  where populate :: Ord a M.Map a $Z$ $\rightarrow$ [a] $\rightarrow$ M.Map a Int
        populate m [] = m
        populate m (x:xs) = let freq = (M.findWithDefault 0 x m) :: ℤ
                            in populate (M.insert x (freq + 1) m) xs
\end{lstlisting}
Once the required lists are generated using these they are saved to a file in
the directory \texttt{model}.

\paragraph{\texttt{tag}}

The executable \texttt{run-tag} is where the actual tagging happens. The
frequencies we have pre-computed in \texttt{create-model} are passed in to the
probability function which we give below.
\begin{lstlisting}[
  language=Haskell,
  mathescape,
  columns=fullflexible,
  basicstyle=\small\ttfamily
]
probability :: (Ord a, Ord b) $\Rightarrow$ (a, b) $\rightarrow$ Map a Int $\rightarrow$ Map (a, b) ℤ $\rightarrow \mathscr{L}$
probability (x, y) c$_1$ c$_2$ = if xCount == 0 || yCount == 0
                                   then logFloat $\epsilon$
                                   else (logFloat yCount) / (logFloat xCount)
  where yCount = fromIntegral (findWithDefault 0 (x, y) c₂) ∷ Double
        xCount = fromIntegral (findWithDefault 0 x c₁) ∷ Double
\end{lstlisting}

\texttt{probability} when combined with \texttt{freqMap} yield an efficient
basis for implementing Bayes' law. When we are creating the functions
required by \texttt{HMM} we are storing the map returned by a call to
\texttt{freqMap} in the lexical closure of \texttt{probability} by making use
Haskell's currying so that we can eloquently define the two crucial functions
as follows:
\begin{lstlisting}[
  language=Haskell,
  mathescape,
  columns=fullflexible,
  basicstyle=\small\ttfamily
]
transFn s$_1$ s$_2$ = probability (s$_1$, s$_2$) tagFreqs tagBigramFreqs
outFn s e = probability (e, s) wordFreqs taggedWordFreqs
\end{lstlisting}

\subsection{Accuracy}
\label{subsec:accuracy}

I have tested the POS tagger with 6075 words from a small portion of the corpus
that I reserved for testing (i.e., did not use for tranining the HMM). The POS
tagger tagged 4514 of these words correctly (74.3 \%). Here is a brief list of
known problems:
\begin{description}
  \item[Duplications:] The Turkish language contains lots of duplications.
  Syntactically speaking, a duplication behaves like an word itself. In the
  treebank, reduplication are denoted as as single word composed of joining the
  two halves of the duplication with an underscore (half$_1$\_$half_2$). Since
  there is not an easy way to detect whether two sequential words are
  reduplications or not, I am giving duplications in an incorrect format to the
  HMM. I am predicting (based on my informal tests with the model) that
  correction of this problem would increase the accuracy by about $\%5$.
  \item[Relaxed word order:] Turkish is generally considered a free constituent
  order language \citep{erguvanli, dincer2008suffix}, although there does not seem to be an
  agreed-upon view on this. Yet, it would be safe to say that the word order of
  Turkish is more relaxed compared to English. The HMM-based approach to POS
  tagging presupposes the ordered-ness of the language. Although there is a
  canonical word order in Turkish, it tends not to be rigidly followed all the
  time---the morphosyntax of Turkish allows variation. It might be possible to
  increase the accuracy of the HMM by, for example, isolating bigrams in
  dependent clauses from those in their parents might work since Turkish, due to
  its relaxed order, permits lots of dependent clauses.
\end{description}

\subsection{Performance}
Parsing the entire corpus and creating a model out of it takes 10.5 seconds on
average. Tagging a sentence (which we are assuming to have 5 words on average)
takes $~0.3$ seconds. The implementation of the \texttt{Data.HMM} datatype uses
\texttt{Data.Array}
\footnote{https://hackage.haskell.org/package/array-0.5.1.0/docs/Data-Array.html}
which is why it runs reasonably fast.

\subsection{Limitations}
There is a fundamental limitation to the POS tagger I built: Turkish morphology
is essential to the overall completeness of Turkish.

In English we entertain the notion of \emph{syntactic correctness}, which is
more or less a formalism of correct ordering of words. A speaker of English that has
synactic competence in the language \citep{bachman1990} may know that a sentence
is well-formed, relying on his knowledge of syntactic rules. However, for the
\textit{words}, knowledge of morphological rules is not adequate to conclude
that a word is made up of correctly ordered morphemes; they have to rely on
their lexical knowledge to verify that the morphological form they
derived/inflected is a valid one. Consider the word ``satisfiableness'' which
is not a valid word according to several dictionaries, although being derived
with valid suffixes. The correct form of this concept is ``satisfiability''.

In Turkish, the concept of word validness is a lot like syntactic correctness;
the speaker may rely their knowledge of morphological rules to \emph{generate}
new words. In fact Turkish has \textbf{recursive} morphology \citep{kabak2001}
so a word can be inflected/derived in into infinitely many different forms. This
is why computational processing of Turkish can never fully escape its
morphology.

The hidden markov model I've built for this project \emph{ignores} morphology.

\section{Conclusion}

% Delete this todo
\todo[inline]{Write a concluding summary. Also indicate what future work may be warranted. This section should be at least half a page in length.}


\bibliographystyle{apalike}
\bibliography{your_bibliography}

\end{document}