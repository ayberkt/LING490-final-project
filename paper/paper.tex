\documentclass{article}
\usepackage{natbib}
\usepackage{todonotes}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{mathrsfs}

\newcommand{\hmmURL}{https://hackage.haskell.org/package/hmm-0.2.1.1/docs/Data-HMM.html}

\title{\bf An HMM-based part-of-speech tagger for Turkish}
\author{Ayberk Tosun}
\date{}

\begin{document}
%include agda.fmt
\maketitle

\section{Motivation}
\label{sec:motivation}
My main motivation for undertaking this project is to understand the
principles underlying POS-tagging. Due to time and resource constraints, it is
not possible for me to build a state-of-the-art POS-tagger for
Turkish. Nevertheless, I expect my POS-tagger
to work with a decent accuracy that is adequate to be a starting point for
further development. Moreover, \citet{Korkut2015} began the development of a
Turkish NLP framework named ``Guguk,'' implemented in the programming language Haskell. I intend
to merge this project to Guguk, hence contributing towards
the existence of a Turkish NLP-framework in Haskell.

\section{Related Work}
\label{sec:related_work}

% Briefly survey the most salient prior work that relates to the problem you wish
% to solve. This section should cite relevant sources. This section should be at
% least one to two paragraphs in length.
The most important advancement in the computational linguistics of Turkish is
probably the NLP framework ``Zemberek'' by \citet{akin2007zemberek}, which has been
immensely useful for both the theory and practice of Turkish NLP. Although it started
out as an NLP tool for the Turkish language, Zemberek moved on to become a
a tool for processing all Turkic languages.

The widely cited \citet{oflazer1994tagging} outlines the implementation of a
tool for both morpheme glossing and POS tagging, that has ``98-99 \%'' accuracy. This
tool is based on a morphological specification of Turkish and makes use of a
morphological processor called ``PC-Kimmo'' \citep{antworth1991pc}.

In general, most of the literature relating to the computational processing of Turkish
pertain primarily to morphology---compared to the complexity of Turkish
morphology, its syntax seems to be a less interesting problem.
\section{Proposed Work}

I will implement a part-of-speech tagger for the Turkish language. As
mentioned in section~\ref{sec:related_work}, I will mostly focus on parts-of-speech
of words\footnote{I base my definition of a ``word'' on the orthographic space in
  most of the cases. There are certain exceptions to this, such as
  reduplication which I denote with the POS tag ``Dup''} and will not be
accounting for morphology, unlike
the case with most of the serious research on Turkish NLP. There definitely are
problems with this approach---but fixing these would go beyond the scope of
this class

For programming the POS tagger I will be using Haskell, a statically-typed and purely
functional programming language, along with the
\href{\hmmURL}{\texttt{Data.HMM}} package.
The data, that is probably the most important component of this
project, comes from the METU-Sabanc{\i} treebank built by
\citet{oflazer2003building}.  I am planning to strip the POS tag for each word from
the treebank and use those to train the HMM

\section{Results}
The POS tagger I've built as proposed can successfully tag words with a
reasonable accuracy.

\subsection{A Brief Overview on Implementation}
The project consists of two executables: (1) \texttt{create-model} that reads
the data from the corpus and parses it into a form from which we can easily
construct an \texttt{HMM}. Unfortunately, it is highly impractical---if not
impossible---to directly serialize \texttt{HMM} since it has three fields
containing functions
\footnote{http://stackoverflow.com/questions/17785916/can-haskell-functions
  -be-serialized for further information.}. We consider this a fault in the
design of the \texttt{Data.HMM} package and urge further development. Although
this has almost no effect on performance, it obfuscates the code.

Since processing the corpus is a resource-demanding operation,
\texttt{create-model} is intended be run only once after which \texttt{run-tag}
can be used to make use of that data.

\paragraph{\texttt{create-model}}
The executable \texttt{create-model} is implemented in two files:
\texttt{Parse.hs} and \texttt{Main.hs}. \texttt{Parse.hs} handles the parsing of
the corpus.

The function \texttt{freqMap} takes as input a list of orderable elements
(denoted with \texttt{Ord a}) and creates a \texttt{Map} containing, as value,
the frequency of each element.

\begin{lstlisting}[
  language=Haskell,
  mathescape,
  columns=fullflexible,
  basicstyle=\small\ttfamily
]
freqMap :: Ord a $\Rightarrow$ [a] $\rightarrow$ M.Map a Int
freqMap unigrams = populate M.empty unigrams
  where populate :: Ord a M.Map a $Z$ $\rightarrow$ [a] $\rightarrow$ M.Map a Int
        populate m [] = m
        populate m (x:xs) = let freq = (M.findWithDefault 0 x m) :: ℤ
                            in populate (M.insert x (freq + 1) m) xs
\end{lstlisting}

This is the basis of our \emph{supervised} learning.

\paragraph{\texttt{tag}}

The executable \texttt{run-tag} is where the actual tagging happens. The
frequencies we have pre-computed in \texttt{create-model} are passed in to the
probability function which we give below.
\begin{lstlisting}[
  language=Haskell,
  mathescape,
  columns=fullflexible,
  basicstyle=\small\ttfamily
]
probability :: (Ord a, Ord b) $\Rightarrow$ (a, b) $\rightarrow$ Map a Int$\rightarrow$ Map (a, b) ℤ $\rightarrow \mathscr{L}$
probability (x, y) c$\_1$ c$\_2$ = if xCount == 0 || yCount == 0
                           then logFloat $\epsilon$
                           else (logFloat yCount) / (logFloat xCount)
  where yCount = fromIntegral (findWithDefault 0 (x, y) c₂) ∷ Double
        xCount = fromIntegral (findWithDefault 0 x c₁) ∷ Double
\end{lstlisting}
\subsection{Performance}
Parsing the entire corpus and creating a model out of it takes 10.5 seconds on
average. Tagging a sentence (which we're assuming to have 5 words on average)
takes $~0.3$ seconds. The implementation of the \texttt{Data.HMM} datatype uses
\texttt{Data.Array}
\footnote{https://hackage.haskell.org/package/array-0.5.1.0/docs/Data-Array.html}
which is why it runs reasonably fast.

\subsection{Limitations}
There is a fundamental limitation to the POS tagger I built: Turkish morphology
is essential to the overall completeness of Turkish.

In English we entertain the notion of \emph{syntactic correctness}, which is
more or less a formalism of correct ordering of words. A speaker of English that has
synactic competence in the language \citep{bachman1990} may know that a sentence
is well-formed, relying on his knowledge of syntactic rules. However, for the
\textit{words}, knowledge of morphological rules is not adequate to conclude
that a word is made up of correctly ordered morphemes; they have to rely on
their lexical knowledge to verify that the morphological form they
derived/inflected is a valid one. Consider the word ``satisfiableness'' which
is not a valid word according to several dictionaries, although being derived
with valid suffixes. The correct form of this concept is ``satisfiability''.

In Turkish, the concept of word validness is a lot like syntactic correctness;
the speaker may rely their knowledge of morphological rules to \emph{generate}
new words. In fact Turkish has \textbf{recursive} morphology \citep{kabak2001}
so a word can be inflected/derived in into infinitely many different forms. This
is why computational processing of Turkish can never fully escape its
morphology.

The hidden markov model I've built for this project \emph{ignores} morphology.

\section{Conclusion}

% Delete this todo
\todo[inline]{Write a concluding summary. Also indicate what future work may be warranted. This section should be at least half a page in length.}


\bibliographystyle{apalike}
\bibliography{your_bibliography}

\end{document}